{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     16\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      2\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /miniconda/envs/spot-gpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "It looks like you are subclassing `Model` and you forgot to call `super(YourClass, self).__init__()`. Always start with this line.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m/miniconda/envs/spot-gpu/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                 \u001b[0mis_graph_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/notebook/chuanqi/pose/Mask_RCNN/mrcnn/parallel_model.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, attrname)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParallelModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ParallelModel' object has no attribute '_is_graph_network'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7928c4edfc77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create model in training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model = modellib.MaskRCNN(mode=\"training\", config=config,\n\u001b[0;32m----> 3\u001b[0;31m                           model_dir=MODEL_DIR)\n\u001b[0m",
      "\u001b[0;32m~/data/notebook/chuanqi/pose/Mask_RCNN/mrcnn/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mode, config, model_dir)\u001b[0m\n\u001b[1;32m   1835\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_log_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1837\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/notebook/chuanqi/pose/Mask_RCNN/mrcnn/model.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, mode, config)\u001b[0m\n\u001b[1;32m   2060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGPU_COUNT\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mmrcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallelModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2062\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallelModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGPU_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/notebook/chuanqi/pose/Mask_RCNN/mrcnn/parallel_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, keras_model, gpu_count)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mgpu_count\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mGPUs\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mMust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \"\"\"\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpu_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mmerged_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/spot-gpu/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 raise RuntimeError(\n\u001b[0;32m--> 310\u001b[0;31m                     \u001b[0;34m'It looks like you are subclassing `Model` and you '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m                     \u001b[0;34m'forgot to call `super(YourClass, self).__init__()`.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                     ' Always start with this line.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: It looks like you are subclassing `Model` and you forgot to call `super(YourClass, self).__init__()`. Always start with this line."
     ]
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spot-gpu]",
   "language": "python",
   "name": "conda-env-spot-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
