{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /miniconda/envs/spot-gpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /root/data/notebook/chuanqi/pose/Mask_RCNN/logs/shapes20190504T1223/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "WARNING:tensorflow:From /miniconda/envs/spot-gpu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda/envs/spot-gpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/miniconda/envs/spot-gpu/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spot-gpu]",
   "language": "python",
   "name": "conda-env-spot-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
